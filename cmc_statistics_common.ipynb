{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Это код для подсчета различных статистик, связанных с идентификацией ключевых для различных учебных потоков слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "txtsDir = '../../txt_cmc_diploms/3/'\n",
    "txts = [filename for filename in os.listdir(txtsDir) if filename != 'broken_encoding']\n",
    "\n",
    "print(txts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частота встречаемости слова в корпусе работ, соотвествующих учебному потоку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from pyrutok import Token, Sentence, Tokenizer, GraphemTag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "morphAnalyzer = MorphAnalyzer()\n",
    "corpusWords = Counter()\n",
    "\n",
    "for txtFilename in txts:\n",
    "    with open(txtsDir+txtFilename, 'r') as txtFile:\n",
    "        content = txtFile.read()\n",
    "        \n",
    "        for sentence in Tokenizer(content):\n",
    "            for token in sentence:\n",
    "                if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                    word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                    if word not in stopwords.words('russian'):\n",
    "                        corpusWords[word] += 1\n",
    "        \n",
    "print(corpusWords.most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = 0\n",
    "for word in corpusWords:\n",
    "    wordCount += corpusWords[word]\n",
    "\n",
    "for word in corpusWords.most_common(200):\n",
    "    print(word[1], '/', wordCount, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Факт использования слова в корпусе работ, соотвествующих учебному потоку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from pyrutok import Token, Sentence, Tokenizer, GraphemTag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "morphAnalyzer = MorphAnalyzer()\n",
    "corpusWords = []\n",
    "\n",
    "for txtFilename in txts:\n",
    "    with open(txtsDir+txtFilename, 'r') as txtFile:\n",
    "        content = txtFile.read()\n",
    "        words = []\n",
    "        \n",
    "        for sentence in Tokenizer(content):\n",
    "            for token in sentence:\n",
    "                if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                    word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                    if word not in stopwords.words('russian'):\n",
    "                        words.append(word)\n",
    "        corpusWords = corpusWords + list(set(words))\n",
    "        \n",
    "print(Counter(corpusWords).most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in Counter(corpusWords).most_common(200):\n",
    "    print(word[1], '/', 124, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency-inverse document frequency (TF-IDF) слова в корпусе работ, соотвествующих учебному потоку.\n",
    "\n",
    "<ins>Примечание</ins>: *это способ оценить важность термина для какого-либо документа относительно всех остальных документов. Принцип такой — если слово встречается в каком-либо документе часто, при этом встречаясь редко во всех остальных документах — это слово имеет большую значимость для того самого документа.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from pyrutok import Token, Sentence, Tokenizer, GraphemTag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "corpusTf = []\n",
    "corpusWords = set()\n",
    "\n",
    "for txtFilename in txts:\n",
    "    documentWords = Counter()\n",
    "    with open(txtsDir+txtFilename, 'r') as txtFile:\n",
    "        contentString = txtFile.read()\n",
    "        contentList = []\n",
    "        for sentence in Tokenizer(contentString):\n",
    "            for token in sentence:\n",
    "                if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                    word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                    if word not in stopwords.words('russian'):\n",
    "                        contentList.append(word)\n",
    "                        \n",
    "        contentTf = Counter(contentList)\n",
    "        for word in contentTf:\n",
    "            contentTf[word] = contentTf[word] / float(len(contentList))\n",
    "            corpusWords.add(word)\n",
    "        \n",
    "        corpusTf.append(contentTf)\n",
    "      \n",
    "corpusIdf = {}\n",
    "for word in corpusWords:\n",
    "    corpusIdf[word] = math.log10(len(corpusTf) / sum([1 for document in corpusTf if word in document]))\n",
    "           \n",
    "for document in corpusTf:\n",
    "    for word in document:\n",
    "        document[word] = document[word] * corpusIdf[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "        \n",
    "pprint(corpusTf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Это код для извлечения набора наиболее репрезентативных для выпускных работ различных учебных потоков слов.\n",
    "\n",
    "Метод взят из статьи *The automated acquisition of topic signatures for text summarization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "relevantTxtsDir = '../../txt_cmc_diploms/2/'\n",
    "nonRelevantTxtsDirs = ['../../txt_cmc_diploms/1/', '../../txt_cmc_diploms/3/']\n",
    "\n",
    "relevantTxts = [relevantTxtsDir + filename for filename in os.listdir(relevantTxtsDir) if filename != 'broken_encoding']\n",
    "nonRelevantTxts = []\n",
    "for nonRelevantTxtsDir in nonRelevantTxtsDirs:\n",
    "    nonRelevantTxts += [nonRelevantTxtsDir + filename for filename in os.listdir(nonRelevantTxtsDir) if filename != 'broken_encoding']\n",
    "\n",
    "print(relevantTxts)\n",
    "print(nonRelevantTxts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from pyrutok import Token, Sentence, Tokenizer, GraphemTag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "    \n",
    "morphAnalyzer = MorphAnalyzer()\n",
    "\n",
    "# подсчет частоты встречаемости слов в работах одного учебного потока\n",
    "relevantFreq = Counter()\n",
    "\n",
    "for txtFilename in relevantTxts:\n",
    "    with open(txtFilename, 'r') as txtFile:\n",
    "        contentString = txtFile.read()\n",
    "        for sentence in Tokenizer(contentString):\n",
    "            for token in sentence:\n",
    "                if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                    word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                    if word not in stopwords.words('russian'):\n",
    "                        relevantFreq[word] += 1\n",
    "\n",
    "# подсчет частоты встречаемости слов в работах двух других учебных потоков\n",
    "nonRelevantFreq = Counter()\n",
    "\n",
    "for txtFilename in nonRelevantTxts:\n",
    "    with open(txtFilename, 'r') as txtFile:\n",
    "        contentString = txtFile.read()\n",
    "        for sentence in Tokenizer(contentString):\n",
    "            for token in sentence:\n",
    "                if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                    word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                    if word not in stopwords.words('russian'):\n",
    "                        nonRelevantFreq[word] += 1\n",
    "\n",
    "# подсчет частоты встречаемости остальных слов в работах одного учебного потока\n",
    "notRelevantFreq = Counter()\n",
    "\n",
    "for word in relevantFreq:\n",
    "    notRelevantFreq[word] = sum([relevantFreq[notWordFreq] for notWordFreq in relevantFreq if notWordFreq != word])\n",
    "\n",
    "# подсчет частоты встречаемости остальных слов в работах двух других учебного потока\n",
    "notNonRelevantFreq = Counter()\n",
    "\n",
    "for word in nonRelevantFreq:\n",
    "    notNonRelevantFreq[word] = sum([nonRelevantFreq[notWordFreq] for notWordFreq in nonRelevantFreq if notWordFreq != word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from pyrutok import Token, Sentence, Tokenizer, GraphemTag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "    \n",
    "morphAnalyzer = MorphAnalyzer()\n",
    "\n",
    "relevantTxtsWithWord = {}\n",
    "for requiredWord in relevantFreq:\n",
    "    relevantTxtsWithWord[requiredWord] = Counter()\n",
    "\n",
    "# количество релевантных документов с рассматриваемым словом\n",
    "for txtFilename in relevantTxts:\n",
    "    with open(txtFilename, 'r') as txtFile:\n",
    "        contentString = txtFile.read()\n",
    "        for sentence in Tokenizer(contentString):\n",
    "            for token in sentence:\n",
    "                if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                    word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                    if word not in stopwords.words('russian') and word in relevantFreq:\n",
    "                        relevantTxtsWithWord[word][txtFilename] = 1\n",
    "\n",
    "nonRelevantTxtsWithWord = {}\n",
    "for requiredWord in relevantFreq:\n",
    "    nonRelevantTxtsWithWord[requiredWord] = Counter()\n",
    "    \n",
    "# количество нерелевантных документов с рассматриваемым словом\n",
    "for txtFilename in nonRelevantTxts:\n",
    "    with open(txtFilename, 'r') as txtFile:\n",
    "        contentString = txtFile.read()\n",
    "        for sentence in Tokenizer(contentString):\n",
    "            for token in sentence:\n",
    "                if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                    word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                    if word not in stopwords.words('russian') and word in relevantFreq:\n",
    "                        nonRelevantTxtsWithWord[word][txtFilename] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantFreqNormalized = Counter()\n",
    "relevantFreqWordsCount = sum([relevantFreq[word] for word in relevantFreq])\n",
    "for requiredWord in relevantFreq:\n",
    "    relevantFreqNormalized[requiredWord] = relevantFreq[requiredWord] / relevantFreqWordsCount\n",
    "    \n",
    "nonRelevantFreqNormalized = Counter()\n",
    "nonRelevantFreqWordsCount = sum([nonRelevantFreq[word] for word in nonRelevantFreq])\n",
    "for requiredWord in nonRelevantFreq:\n",
    "    nonRelevantFreqNormalized[requiredWord] = nonRelevantFreq[requiredWord] / nonRelevantFreqWordsCount\n",
    "    \n",
    "notRelevantFreqNormalized = Counter()\n",
    "notRelevantFreqWordsCount = sum([relevantFreq[word] for word in relevantFreq])\n",
    "for requiredWord in relevantFreq:\n",
    "    notRelevantFreqNormalized[requiredWord] = notRelevantFreq[requiredWord] / notRelevantFreqWordsCount\n",
    "    \n",
    "notNonRelevantFreqNormalized = Counter()\n",
    "notNonRelevantFreqWordsCount = sum([nonRelevantFreq[word] for word in nonRelevantFreq])\n",
    "for requiredWord in nonRelevantFreq:\n",
    "    notNonRelevantFreqNormalized[requiredWord] = notNonRelevantFreq[requiredWord] / notNonRelevantFreqWordsCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.special\n",
    "from decimal import Decimal\n",
    "    \n",
    "def binomialDistribution(k, n, x):\n",
    "    return Decimal(scipy.special.binom(n, k)) * pow(x, Decimal(k)) * pow(Decimal(1) - x, Decimal(n - k))\n",
    "\n",
    "# рассчет весов в соответствии со статьей\n",
    "termsWeightEstimation = Counter()\n",
    "for requiredWord in relevantFreq:\n",
    "    o11 = relevantFreqNormalized[requiredWord]\n",
    "    o12 = nonRelevantFreqNormalized[requiredWord] if requiredWord in nonRelevantFreqNormalized else 1\n",
    "    o21 = notRelevantFreqNormalized[requiredWord]\n",
    "    o22 = notNonRelevantFreqNormalized[requiredWord] if requiredWord in notNonRelevantFreqNormalized else 1\n",
    "    p = Decimal(len(relevantTxts) / (len(relevantTxts) + len(nonRelevantTxts)))\n",
    "    \n",
    "    p1 = Decimal(len(relevantTxtsWithWord[requiredWord]) / (len(relevantTxtsWithWord[requiredWord]) + len(nonRelevantTxtsWithWord[requiredWord]) + 1))\n",
    "    p2 = Decimal((len(nonRelevantTxtsWithWord[requiredWord]) + 1) / (len(relevantTxtsWithWord[requiredWord]) + len(nonRelevantTxtsWithWord[requiredWord]) + 1))\n",
    "\n",
    "    # рассчет функции правдоподобия первой гипотезы\n",
    "    likelihoodH1 = binomialDistribution(o11, o11+o12, p) * binomialDistribution(o21, o21+o22, p)\n",
    "\n",
    "    # рассчет функции правдоподобия второй гипотезы\n",
    "    likelihoodH2 = binomialDistribution(o11, o11+o12, p1) * binomialDistribution(o21, o21+o22, p2)\n",
    "    \n",
    "#     if likelihoodH2 == 0:\n",
    "#         print(requiredWord, o11, o12, o21, o22, p, p1, p2)\n",
    "#         continue\n",
    "    \n",
    "    termsWeightEstimation[requiredWord] = -2 * (likelihoodH1 / likelihoodH2).log10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(termsWeightEstimation.most_common(30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
