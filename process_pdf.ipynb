{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    %reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    %run common_info.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализуем извлечение важных предложений на всех уровнях документа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run process_pdf_title.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run extract_headers_text_structure.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделяем заголовки, которые присутствуют на каждом уровне иерархии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitHeadersByLevels(headersAndTextHierarchy, level, headersByLevels, prevLevelNumeration):\n",
    "    if level + 1 not in headersByLevels:\n",
    "        headersByLevels[level + 1] = []\n",
    "    levelHeaders = []\n",
    "    for levelNumeration, nestedHierarchy in headersAndTextHierarchy.items():\n",
    "        numerationDelimiter = '' if prevLevelNumeration == '' else '.'\n",
    "        delimiter = '' if prevLevelNumeration == '' and levelNumeration == '' else ' '\n",
    "        headersByLevels[level + 1] += splitHeadersByLevels(nestedHierarchy[1],\n",
    "                                                           level + 1,\n",
    "                                                           headersByLevels,\n",
    "                                                           prevLevelNumeration + numerationDelimiter + levelNumeration)\n",
    "        if isinstance(nestedHierarchy[0], list):\n",
    "            for headerIndex in range(0, nestedHierarchy[0]):\n",
    "                levelHeaders.append(prevLevelNumeration + numerationDelimiter + levelNumeration + delimiter + nestedHierarchy[0][headerIndex]['header'])\n",
    "        else:\n",
    "            levelHeaders.append(prevLevelNumeration + numerationDelimiter + levelNumeration + delimiter + nestedHierarchy[0]['header'])\n",
    "    return levelHeaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "draftHeadersByLevels = {}\n",
    "level = 1\n",
    "draftHeadersByLevels[1] = splitHeadersByLevels(headersAndTextHierarchy, level, draftHeadersByLevels, '')\n",
    "\n",
    "headersByLevels = {}\n",
    "for level in draftHeadersByLevels:\n",
    "    if draftHeadersByLevels[level] != []:\n",
    "        headersByLevels[level] = draftHeadersByLevels[level]\n",
    "draftHeadersByLevels = None\n",
    "\n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для каждого заголовка на каждом уровне иерархии определяем все вложенные заголовки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractHeaderNestedHeaders(headersAndTextHierarchy, requiredHeader, extract, prevLevelNumeration):\n",
    "    if not headersAndTextHierarchy:\n",
    "        return []\n",
    "    nestedHeaders = []\n",
    "    for levelNumeration, nestedHierarchy in headersAndTextHierarchy.items():\n",
    "        numerationDelimiter = '' if prevLevelNumeration == '' else '.'\n",
    "        delimiter = '' if prevLevelNumeration == '' and levelNumeration == '' else ' '\n",
    "        levelNumeration = prevLevelNumeration + numerationDelimiter + levelNumeration\n",
    "        if isinstance(nestedHierarchy[0]['header'], list):\n",
    "            for headerIndex in range(0, nestedHierarchy[0]):\n",
    "                header = levelNumeration + delimiter + nestedHierarchy[0][headerIndex]['header']\n",
    "                nestedHeaders += extractHeaderNestedHeaders(nestedHierarchy[1], requiredHeader, False, levelNumeration)\n",
    "        else:\n",
    "            header = levelNumeration + delimiter + nestedHierarchy[0]['header']\n",
    "            if extract:\n",
    "                nestedHeaders += [header] + extractHeaderNestedHeaders(nestedHierarchy[1], requiredHeader, extract, levelNumeration)\n",
    "            if not extract and header == requiredHeader:\n",
    "                nestedHeaders += extractHeaderNestedHeaders(nestedHierarchy[1], requiredHeader, True, levelNumeration)\n",
    "            nestedHeaders += extractHeaderNestedHeaders(nestedHierarchy[1], requiredHeader, False, levelNumeration)\n",
    "    return nestedHeaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "headersAndNestedHeadersByLevels = OrderedDict()\n",
    "for level in headersByLevels:\n",
    "    headersAndNestedHeadersByLevels[level] = []\n",
    "    for header in headersByLevels[level]:\n",
    "        headersAndNestedHeadersByLevels[level].append({\n",
    "            'header': header, \n",
    "            'nestedHeaders': extractHeaderNestedHeaders(headersAndTextHierarchy, header, False, '')\n",
    "        })\n",
    "        \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndNestedHeadersByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделяем текст, который соотвествует заголовкам на каждом уровне иерархии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTextByHeadersAndLevels(headerPattern, nestedHeadersPatterns, headerPatternsAndText):\n",
    "    text = ''\n",
    "    for headerPatternAndText in headerPatternsAndText:\n",
    "        if headerPatternAndText['headerPattern'] == headerPattern or \\\n",
    "           headerPatternAndText['headerPattern'] in nestedHeadersPatterns:\n",
    "            text += headerPatternAndText['text'] + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "headersAndTextByLevels = OrderedDict()\n",
    "for level, headersAndNestedHeaders in headersAndNestedHeadersByLevels.items():\n",
    "    headersAndTextByLevels[level] = []\n",
    "    for headerAndNestedHeaders in headersAndNestedHeaders:\n",
    "        headersAndTextByLevels[level].append({\n",
    "            'header': headerAndNestedHeaders['header'],\n",
    "            'text': splitTextByHeadersAndLevels(headerAndNestedHeaders['header'],\n",
    "                                                headerAndNestedHeaders['nestedHeaders'],\n",
    "                                                headerPatternsAndText)\n",
    "        })\n",
    "        \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndTextByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавляем нулевой уровень иерархии — уровень документа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "headersAndTextByLevelsCopy = headersAndTextByLevels\n",
    "headersAndTextByLevels = OrderedDict()\n",
    "headersAndTextByLevels[0] = []\n",
    "headersAndTextByLevels[0].append({\n",
    "    'header': titlePageDict['workTitle'],\n",
    "    'text': ''\n",
    "})\n",
    "for headerAndText in headersAndTextByLevelsCopy[1]:\n",
    "    headersAndTextByLevels[0][0]['text'] += headerAndText['text']\n",
    "for level, headersAndText in headersAndTextByLevelsCopy.items():\n",
    "    headersAndTextByLevels[level] = []\n",
    "    for headerAndText in headersAndText:\n",
    "        headersAndTextByLevels[level].append(headerAndText)\n",
    "  \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndTextByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бъем текст на предложения и определяем коэффициент этого предложения в тексте в соответствии с эвристиками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "introductoryWords = set(['итак', 'следовательно', 'значит', 'наоборот', 'напротив', 'далее', 'наконец', 'впрочем', 'например', 'главное', 'кстати'])\n",
    "introductoryBigrams = set(['в заключение', 'заметим что', 'между прочим', 'в общем', 'в частности', 'прежде всего', 'кроме того', 'сверх того', 'стало быть', 'к примеру', 'таким образом'])\n",
    "introductoryTrigrams = set(['с одной стороны', 'с другой стороны'])\n",
    "\n",
    "discursiveMarkers = set(['причина', 'ведь', 'поэтому'])\n",
    "discursiveBigrams = set(['потому что', 'быть причина', 'стать причина', 'являться причина'])\n",
    "\n",
    "advantagesMarkers = set(['достоинство', 'недостаток', 'плюс', 'минус'])\n",
    "\n",
    "introductionWords = set(['актуальность', 'актуальный'])\n",
    "introductionBigrams = set(['цель работа'])\n",
    "\n",
    "bulletListMarkers = set(['во-первых', 'во-вторых', 'в-третьих', 'в-четвертых', 'в-четвёртых'])\n",
    "bulletMarkersPattern = 'во\\-первых|во\\-вторых|в\\-третьих|в\\-четвертых|в\\-четвёртых'\n",
    "\n",
    "imageReferencesMarkers = ['рис', 'рисунок']\n",
    "imageReferencesPattern = 'рис|рисунок'\n",
    "tableReferencesMarkers = ['таблица', 'табла', 'таб']\n",
    "tableReferencesPattern = 'таблица|табла|таб'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run extract_possible_terms.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrutok import Token, Sentence, Tokenizer, GraphemTag, TokenType\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "morphAnalyzer = MorphAnalyzer()\n",
    "imageNumerations = set([caption['numeration'] for caption in captionsToImages])\n",
    "tableNumerations = set([caption['numeration'] for caption in captionsToTables])\n",
    "\n",
    "headersAndSentencesByLevels = {}\n",
    "for level, headersAndText in headersAndTextByLevels.items():\n",
    "    headersAndSentencesByLevels[level] = []\n",
    "    for blockIndex in range(0, len(headersAndText)):\n",
    "        headersAndSentencesByLevels[level].append({\n",
    "            'header': headersAndText[blockIndex]['header'],\n",
    "            'sentences': []\n",
    "        })\n",
    "        imageReferences = set()\n",
    "        tableReferences = set()   \n",
    "        for tempText in headersAndText[blockIndex]['text'].split('\\n'):\n",
    "            tempText = tempText.replace('т.д.', 'так далее').replace('т. д.', 'так далее')\n",
    "            tempText = tempText.replace('т.п.', 'тому подобное.').replace('т. п.', 'тому подобное.')\n",
    "            tempText = tempText.replace('т.е.', 'то есть').replace('т. е.', 'то есть').replace('т.е', 'то есть')\n",
    "    #         tempText = tempText.replace('\\n', '\\n\\n')\n",
    "            tempText = tempText.replace(':\\n', '?? ').replace(': \\n', '?? ').replace('.\\n', '!! ').replace('. \\n', '!! ')\n",
    "            tempText = tempText.replace(',\\n', ', ').replace(', \\n', ', ').replace('\\n', '. ')\n",
    "            tempText = tempText.replace('Рис.', 'Рис_').replace('рис.', 'рис_').replace('англ.', 'англ_').replace('см.', 'см_').replace('См.', 'См_')\n",
    "            tempText = tempText.replace('Таб.', 'Таб_').replace('таб.', 'таб_').replace('Табл.', 'Табл_').replace('табл.', 'табл_')\n",
    "            tempText = tempText.replace('\\uf0b7', '•')\n",
    "            for url in urls:\n",
    "                tempText = tempText.replace(url, '*URL*')\n",
    "            # многоточия негативно влияют на разделение предложений\n",
    "            tempText = re.sub(r' *\\. *\\. *\\. *', '___', tempText)\n",
    "            # убираем ссылки на литературу из текста\n",
    "            tempText = re.sub(r'\\[(\\d+(,? ?|\\-?|–?))*\\]', '', tempText)\n",
    "            tempText = tempText.replace('  ', ' ').replace(').', '_!').replace('].', '_?')\n",
    "            # почему-то точки приводят токенайзер в замешательство\n",
    "            tempText = re.sub(r'(\\d+)\\.', r'\\1}', tempText)\n",
    "#             for us in unusualSymbols:\n",
    "#                 tempText = tempText.replace(us, '')\n",
    "            for sentence in Tokenizer(tempText):\n",
    "                sentenceText = re.sub(r'(\\d+)\\}', r'\\1.', sentence.as_text())\n",
    "                sentenceText = sentenceText.replace('_!', ').').replace('_?', '].').replace('!!', '.').replace('!', '.').replace('??', ':').replace(':.', ':')\n",
    "                sentenceText = re.sub(r'( *\\.)+', '.', sentenceText)\n",
    "                sentenceText = sentenceText.replace('Рис_', 'Рис.').replace('рис_', 'рис.').replace('англ_', 'англ.').replace('см_', 'см.').replace('См_', 'См.')\n",
    "                sentenceText = sentenceText.replace('Таб_', 'Таб.').replace('таб_', 'таб.').replace('Табл_', 'Табл.').replace('табл_', 'табл.')\n",
    "                sentenceText = sentenceText.replace('___', '...')\n",
    "\n",
    "                sentenceImportance = 1\n",
    "                sentenceImageReferences = set()\n",
    "                sentenceTableReferences = set()\n",
    "\n",
    "                for term in terms:\n",
    "                    if term in sentenceText:\n",
    "                        sentenceImportance = 1.1\n",
    "\n",
    "                sentenceWords = re.sub(r'[' + re.escape(punctuation) + r']', '', sentenceText).lower().split(' ')\n",
    "                sentenceBigrams = [sentenceWords[i] + ' ' + sentenceWords[i+1] for i in range(0, len(sentenceWords)-1)]\n",
    "                sentenceTrigrams = [sentenceWords[i] + ' ' + sentenceWords[i+1] + ' ' + sentenceWords[i+2] \n",
    "                                    for i in range(0, len(sentenceWords)-2)]\n",
    "    #             print(sentenceBigrams)\n",
    "                # проверяем наличие вводных слов и обрезаем их, если они в начале предложения\n",
    "                if set(sentenceWords).intersection(introductoryWords) or \\\n",
    "                   set(sentenceBigrams).intersection(introductoryBigrams) or \\\n",
    "                   set(sentenceTrigrams).intersection(introductoryTrigrams):\n",
    "                    sentenceImportance = 1.1\n",
    "                if len(sentenceWords) > 0 and sentenceWords[0] in introductoryWords:\n",
    "                    sentenceText = sentenceText.split(' ', 1)[1]\n",
    "                    sentenceText = sentenceText[0].upper() + sentenceText[1:]\n",
    "                if len(sentenceBigrams) > 0 and sentenceBigrams[0] in introductoryBigrams:\n",
    "                    sentenceText = sentenceText.split(' ', 2)[2]\n",
    "                    sentenceText = sentenceText[0].upper() + sentenceText[1:]\n",
    "                if len(sentenceTrigrams) > 0 and sentenceTrigrams[0] in introductoryTrigrams:\n",
    "                    sentenceText = sentenceText.split(' ', 3)[3]\n",
    "                    sentenceText = sentenceText[0].upper() + sentenceText[1:]\n",
    "\n",
    "                sentenceTextTemp = sentenceText\n",
    "                for us in unusualSymbols:\n",
    "                    sentenceText = sentenceText.replace(us, '')\n",
    "                sentenceWordsAndNumbersNF = []\n",
    "                for innerSentence in Tokenizer(sentenceText):\n",
    "                    for token in innerSentence:\n",
    "                        if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                            word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                            sentenceWordsAndNumbersNF.append(word)\n",
    "                        if token.get_token_type() == TokenType.NUMBER:\n",
    "                            sentenceWordsAndNumbersNF.append(token.get_escaped_data())\n",
    "                sentenceBigramsNF = [sentenceWordsAndNumbersNF[i] + ' ' + sentenceWordsAndNumbersNF[i+1] \n",
    "                                     for i in range(0, len(sentenceWordsAndNumbersNF)-1)]\n",
    "                sentenceTrigramsNF = [sentenceWordsAndNumbersNF[i] + ' ' + sentenceWordsAndNumbersNF[i+1] + ' ' + sentenceWordsAndNumbersNF[i+2]\n",
    "                                     for i in range(0, len(sentenceWordsAndNumbersNF)-2)]\n",
    "                sentenceText = sentenceTextTemp\n",
    "    #             print(sentenceWordsAndNumbersNF)\n",
    "                # запоминаем ссылки на изображения\n",
    "                for trigram in sentenceTrigramsNF:\n",
    "                    imageReference = re.findall(r'^(' + imageReferencesPattern + r')\\ \\d+\\ \\d+$', trigram)\n",
    "                    if imageReference and trigram.split(' ', 1)[1] in imageNumerations:\n",
    "                        imageReferences.add(trigram.split(' ', 1)[1])\n",
    "                        sentenceImageReferences.add(trigram.split(' ', 1)[1])\n",
    "#                 for bigram in sentenceBigramsNF:\n",
    "#                     imageReference = re.findall(r'^(' + imageReferencesPattern + r')\\ \\d+$', bigram)\n",
    "#                     if imageReference and bigram.split(' ')[1] in imageNumerations:\n",
    "#                         imageReferences.add(bigram.split(' ')[1])\n",
    "#                         sentenceImageReferences.add(bigram.split(' ')[1])\n",
    "#                     # на случай, если человек не подписывает таблицы как таблицы\n",
    "#                     if imageReference and bigram.split(' ')[1] in imageNumerations:\n",
    "#                         imageReferences.add(bigram.split(' ')[1])\n",
    "#                         sentenceImageReferences.add(bigram.split(' ')[1])\n",
    "                # запоминаем ссылки на таблицы\n",
    "                for bigram in sentenceBigramsNF:\n",
    "                    tableReference = re.findall(r'^(' + tableReferencesPattern + r')\\ \\d+$', bigram)\n",
    "                    if tableReference and bigram.split(' ')[1] in tableNumerations:\n",
    "                        tableReferences.add(bigram.split(' ')[1])\n",
    "                        sentenceTableReferences.add(bigram.split(' ')[1])\n",
    "                # проверяем наличие дискурсивных маркеров\n",
    "                if set(sentenceWordsAndNumbersNF).intersection(discursiveMarkers) or \\\n",
    "                   set(sentenceBigramsNF).intersection(discursiveBigrams):\n",
    "                    sentenceImportance = 1.1\n",
    "                # проверяем маркеры, важные для введения\n",
    "                if (set(sentenceWordsAndNumbersNF).intersection(introductionWords) or \\\n",
    "                    set(sentenceBigramsNF).intersection(introductionBigrams)) and \\\n",
    "                   'введение' in headersAndText[blockIndex]['header'].lower():\n",
    "                    sentenceImportance = 1.3\n",
    "                # проверяем возможность сделать предложение частью маркированных списков\n",
    "    #             if set(sentenceWordsAndNumbersNF).intersection(bulletListMarkers):\n",
    "    #                 sentenceText = re.sub(r'(^|,? ?[а-я]* ?)(' + bulletMarkersPattern + r'),', '\\n•', sentenceText, flags=re.IGNORECASE)\n",
    "    #                 # потому что неизвестно, насколько это важно\n",
    "    #                 sentenceImportance = 1.31\n",
    "\n",
    "    #                 if sentenceText[0] == '\\n' and headersAndSentencesByLevels[level][blockIndex]['sentences']:\n",
    "    #                     headersAndSentencesByLevels[level][blockIndex]['sentences'][-1]['sentence'] = \\\n",
    "    #                         headersAndSentencesByLevels[level][blockIndex]['sentences'][-1]['sentence'][-1] + ':'\n",
    "\n",
    "    #                 for sentencePart in sentenceText.split('\\n'):\n",
    "    #                     delimiter = '\\n' if sentencePart != sentenceText.split()[-1] else ''\n",
    "    #                     if sentencePart and sentencePart[0] not in listMarkers and sentencePart[-1] != ':':\n",
    "    #                         sentencePart += ':'\n",
    "    #                     headersAndSentencesByLevels[level][blockIndex]['sentences'].append({\n",
    "    #                         'sentence': sentencePart.replace(' ,', ',').replace(' .', '.').replace(' :', ':').replace(' ;', ';').strip(),\n",
    "    #                         'importance': sentenceImportance,\n",
    "    #                         'betweenBullets': False,\n",
    "    #                         'bulletListPart': True,\n",
    "    #                         'betweenNumbering': False,\n",
    "    #                         'numberedListPart': False,\n",
    "    #                         'imageReferences': sentenceImageReferences,\n",
    "    #                         'tableReferences': sentenceTableReferences\n",
    "    #                     })\n",
    "    #                 continue\n",
    "\n",
    "                if not sentenceText.strip() or len(sentenceText.split(' ')) < 2:\n",
    "                    continue\n",
    "\n",
    "                # пытаемся выбросить записи таблиц            \n",
    "#                 if len([p.start() for p in re.finditer(r'' + '|'.join([re.escape(p) for p in punctuation]) + r'', sentenceText)]) > 20:\n",
    "#                     sentenceImportance = 0\n",
    "\n",
    "                # выбрасываем кривые предложения\n",
    "                if sentenceText[0] in punctuation+'–':\n",
    "                    sentenceImportance = 0\n",
    "                    \n",
    "                # выбрасываем cлишком длинные предложения\n",
    "                if len(sentenceText) > 400:\n",
    "                    sentenceImportance = 0\n",
    "\n",
    "    #             print([p.start() for p in re.finditer(r'' + '|'.join([re.escape(p) for p in punctuation]) + r'', sentenceText)], sentenceText)\n",
    "\n",
    "                headersAndSentencesByLevels[level][blockIndex]['sentences'].append({\n",
    "                    'sentence': sentenceText.replace(' ,', ',').replace(' .', '.').replace(' :', ':').replace(' ;', ';').strip(),\n",
    "                    'importance': sentenceImportance,\n",
    "                    'betweenBullets': False,\n",
    "                    'bulletListPart': False,\n",
    "                    'betweenNumbering': False,\n",
    "                    'numberedListPart': False,\n",
    "                    'imageReferences': sentenceImageReferences,\n",
    "                    'tableReferences': sentenceTableReferences\n",
    "                })\n",
    "                headersAndSentencesByLevels[level][blockIndex]['imageReferences'] = imageReferences\n",
    "                headersAndSentencesByLevels[level][blockIndex]['tableReferences'] = tableReferences\n",
    "\n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndSentencesByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляем значимости предложениям, которые находятся в общей для подразделов текстовой части."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in range(1, len(headersAndNestedHeadersByLevels)):\n",
    "    headersAndNestedHeaders = headersAndNestedHeadersByLevels[level]\n",
    "    for blockIndex in range(0, len(headersAndNestedHeaders)):\n",
    "        headerAndNestedHeaders = headersAndNestedHeaders[blockIndex]\n",
    "        if headerAndNestedHeaders['nestedHeaders']:\n",
    "            firstNestedHeader = headerAndNestedHeaders['nestedHeaders'][0]\n",
    "            firstNestedHeaderNumeration = firstNestedHeader.split(' ')[0].split('.')\n",
    "            firstNestedHeaderLevel = len(firstNestedHeaderNumeration)\n",
    "            # это первое предложение одной из вложенных частей\n",
    "            commonPartEndMarker = next(iter([headerAndSentences \n",
    "                                             for headerAndSentences in headersAndSentencesByLevels[firstNestedHeaderLevel] \n",
    "                                             if headerAndSentences['header'] == firstNestedHeader][0]['sentences']))['sentence']\n",
    "#             print(commonPartEndMarker)\n",
    "            sentenceIndex = 0\n",
    "            sentences = headersAndSentencesByLevels[level][blockIndex]['sentences']\n",
    "            while sentenceIndex != len(sentences) and sentences[sentenceIndex]['sentence'] != commonPartEndMarker:\n",
    "                if sentences[sentenceIndex]['importance'] > 0.5:\n",
    "                    sentences[sentenceIndex]['importance'] = 1.5\n",
    "                sentenceIndex += 1\n",
    "\n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndSentencesByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем безусловно важными предложения, которые изначально являлись частью маркированных списков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level, headersAndSentences in headersAndSentencesByLevels.items():\n",
    "    for headerAndSentences in headersAndSentences:\n",
    "        betweenMarkers = False\n",
    "        betweenNumbering = False\n",
    "        lastMarkedSentenceIndex = None\n",
    "        for sentenceIndex in range(0, len(headerAndSentences['sentences'])):\n",
    "            sentence = headerAndSentences['sentences'][sentenceIndex]\n",
    "            if (sentence['sentence'][0] in listMarkers or \\\n",
    "                (sentence['sentence'][0] not in listMarkers and \\\n",
    "                 sentence['sentence'][-1] == ':')) and \\\n",
    "               sentence['importance'] != 1.31:\n",
    "                if sentence['importance'] > 0.5:\n",
    "                    sentence['importance'] = 2\n",
    "                sentence['bulletListPart'] = True\n",
    "            # с нумерованными списками сложнее, так что выделяем их меньше \n",
    "            if sentence['sentence'][0] in numbers:\n",
    "                if sentence['importance'] > 0.5:\n",
    "                    sentence['importance'] = 2\n",
    "                sentence['numberedListPart'] = True\n",
    "            if sentence['sentence'][0] in listMarkers:\n",
    "                if lastMarkedSentenceIndex:\n",
    "                    for bmsIndex in range(lastMarkedSentenceIndex+1, sentenceIndex):\n",
    "                        headerAndSentences['sentences'][bmsIndex]['betweenBullets'] = True\n",
    "                betweenMarkers = True\n",
    "                lastMarkedSentenceIndex = sentenceIndex\n",
    "            if sentence['sentence'][0] in numbers:\n",
    "                if lastMarkedSentenceIndex:\n",
    "                    for bmsIndex in range(lastMarkedSentenceIndex+1, sentenceIndex):\n",
    "                        headerAndSentences['sentences'][bmsIndex]['betweenNumbering'] = True\n",
    "                betweenNumbering = True\n",
    "                lastMarkedSentenceIndex = sentenceIndex\n",
    "            if sentence['sentence'][-1] == ':':\n",
    "                betweenMarkers = False\n",
    "                betweenNumbering = False\n",
    "                lastMarkedSentenceIndex = None\n",
    "          \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "#     print(listMarkers)\n",
    "    pprint(headersAndSentencesByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вес предложения на определенном уровне иерархии: частотный признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бъем предложения каждого куска текста на каждом уровне иерархии на токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrutok import Token, Sentence, Tokenizer, GraphemTag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "morphAnalyzer = MorphAnalyzer()\n",
    "\n",
    "headersAndWordsByLevels = {}\n",
    "for level, headersAndSentences in headersAndSentencesByLevels.items():\n",
    "    headersAndWordsByLevels[level] = []\n",
    "    for headerAndSentences in headersAndSentences:\n",
    "        words = Counter()\n",
    "        for sentence in headerAndSentences['sentences']:\n",
    "            tempText = sentence['sentence'].replace('*formula placeholder*', '')\n",
    "            for us in unusualSymbols:\n",
    "                tempText = tempText.replace(us, '')\n",
    "            for sentence in Tokenizer(tempText):\n",
    "                for token in sentence:\n",
    "                    if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                        word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                        if word not in stopwords.words('russian'):\n",
    "                            words[word] += 1\n",
    "        headersAndWordsByLevels[level].append({\n",
    "            'header': headerAndSentences['header'],\n",
    "            'words': words\n",
    "        })\n",
    " \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndWordsByLevels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def countWordFrequencyFeature(word, \n",
    "                              wordCount, \n",
    "                              allWordsCount, \n",
    "                              textBlocksAtLevelCount, \n",
    "                              textBlocksWithWordAtLevelCount):\n",
    "    return (wordCount / allWordsCount) * math.log2(textBlocksAtLevelCount * len(word) / textBlocksWithWordAtLevelCount)\n",
    "\n",
    "def countWordsFrequencyFeature(requiredHeaderAndWords, headersAndWordsAtLevel):\n",
    "    wordFrequencyWeights = Counter()\n",
    "    for word in requiredHeaderAndWords['words']:\n",
    "        wordFrequencyWeights[word] = countWordFrequencyFeature(\n",
    "            word,\n",
    "            requiredHeaderAndWords['words'][word],\n",
    "            sum([requiredHeaderAndWords['words'][word] for word in requiredHeaderAndWords['words']]),\n",
    "            len(headersAndWordsAtLevel),\n",
    "            len([headerAndWordsAtLevel for headerAndWordsAtLevel in headersAndWordsAtLevel if word in headerAndWordsAtLevel['words']]))\n",
    "    return wordFrequencyWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "headersAndWordFrequencyWeightsByLevels = {}\n",
    "for level, headersAndWords in headersAndWordsByLevels.items():\n",
    "    headersAndWordFrequencyWeightsByLevels[level] = []\n",
    "    for headerAndWords in headersAndWords:\n",
    "        headersAndWordFrequencyWeightsByLevels[level].append({\n",
    "            'header': headerAndWords['header'],\n",
    "            'wordFrequencyWeights': countWordsFrequencyFeature(headerAndWords, headersAndWords)\n",
    "        })\n",
    "        \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndWordFrequencyWeightsByLevels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrutok import Token, Sentence, Tokenizer, GraphemTag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "\n",
    "def countSentenceFrequencyFeatureByLevels(requiredSentence, wordsFrequencyWeight):\n",
    "    morphAnalyzer = MorphAnalyzer()\n",
    "    sentenceWeight = 0\n",
    "    for us in unusualSymbols:\n",
    "        requiredSentence = requiredSentence.replace(us, '')\n",
    "    for sentence in Tokenizer(requiredSentence):\n",
    "        for token in sentence:\n",
    "            if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                if word not in stopwords.words('russian'):\n",
    "                    sentenceWeight += wordsFrequencyWeight[word]\n",
    "    return sentenceWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for level, headersAndSentences in headersAndSentencesByLevels.items():\n",
    "    for blockIndex in range(0, len(headersAndSentences)):\n",
    "        for sentence in headersAndSentences[blockIndex]['sentences']:\n",
    "            sentenceWeight = countSentenceFrequencyFeatureByLevels(\n",
    "                sentence['sentence'], \n",
    "                headersAndWordFrequencyWeightsByLevels[level][blockIndex]['wordFrequencyWeights']\n",
    "            )\n",
    "            sentence['frequencyWeight'] = sentenceWeight # * sentence['importance']\n",
    "    \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndSentencesByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вес предложения на определенном уровне иерархии: признак местоположения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cчитаем ту часть веса, которую относится к уровню, подразделов и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSectionsPartWeight(headersAndTextHierarchy, requiredHeader):\n",
    "    headerNumeration = requiredHeader.split(' ', -1)[0]\n",
    "    weight = 1\n",
    "    if set(headerNumeration).issubset(numbersAndPoint):\n",
    "        nestedHierarchy = headersAndTextHierarchy\n",
    "        for headerNumerationPart in headerNumeration.split('.'):\n",
    "#             pprint(nestedHierarchy)\n",
    "            weight *= min(\n",
    "                int(headerNumerationPart),\n",
    "                max(\n",
    "                    int(level) for level in nestedHierarchy if level != ''\n",
    "                ) - \\\n",
    "                int(headerNumerationPart) + 1 + \\\n",
    "                ((len(nestedHierarchy[''][0]) if isinstance(nestedHierarchy[''][0], list) else 1) \n",
    "                 if '' in nestedHierarchy \n",
    "                 else 0)\n",
    "            )\n",
    "            nestedHierarchy = nestedHierarchy[headerNumerationPart][1]\n",
    "    else:\n",
    "        if isinstance(headersAndTextHierarchy[''][0], list):\n",
    "            for headerIndex in range(0, len(headersAndTextHierarchy[''][0])):\n",
    "                if headersAndTextHierarchy[''][0][headerIndex] == requiredHeader:\n",
    "                    weight *= min(\n",
    "                        len(headersAndTextHierarchy[''][0]) - headerIndex,\n",
    "                        max(\n",
    "                            int(level) for level in headersAndTextHierarchy if level != ''\n",
    "                        ) + headerIndex + 1\n",
    "                    )\n",
    "        else:\n",
    "            weight *= min(\n",
    "                1,\n",
    "                max(\n",
    "                    int(level) for level in headersAndTextHierarchy if level != ''\n",
    "                ) + 1\n",
    "            )\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for level, headersAndSentences in headersAndSentencesByLevels.items():\n",
    "    if level == 0:\n",
    "        continue\n",
    "    for headerAndSentences in headersAndSentences:\n",
    "        for sentcence in headerAndSentences['sentences']:\n",
    "            headerAndSentences['sectionInverseWeight'] = countSectionsPartWeight(\n",
    "                headersAndTextHierarchy, headerAndSentences['header']\n",
    "            )\n",
    "        \n",
    "for level, headersAndSentences in headersAndSentencesByLevels.items():\n",
    "    if level == 0:\n",
    "        continue\n",
    "    for headerAndSentences in headersAndSentences:\n",
    "        for sentenceIndex in range(0, len(headerAndSentences['sentences'])):\n",
    "            headerAndSentences['sentences'][sentenceIndex]['locationWeight'] = \\\n",
    "                1 / (\n",
    "                min(sentenceIndex + 1, len(headerAndSentences['sentences']) - sentenceIndex) * \\\n",
    "                headerAndSentences['sectionInverseWeight']) # * \\\n",
    "#                  headerAndSentences['sentences'][sentenceIndex]['importance']\n",
    "        \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndSentencesByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вес предложения на определенном уровне иерархии: заголовочный признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определяем вышележащие заголовки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineOverlyingHeaders(headersAndTextHierarchy, requiredHeader):\n",
    "    headerNumeration = requiredHeader.split(' ', -1)[0]\n",
    "    overlyingHeaders = []\n",
    "    overlyingHeaderNumeration = ''\n",
    "    nestedHierarchy = headersAndTextHierarchy\n",
    "    if set(headerNumeration).issubset(numbersAndPoint):\n",
    "        for headerNumerationPart in headerNumeration.split('.')[:-1]:\n",
    "            overlyingHeaders.append(overlyingHeaderNumeration + headerNumerationPart + ' ' + nestedHierarchy[headerNumerationPart][0]['header'])\n",
    "            nestedHierarchy = nestedHierarchy[headerNumerationPart][1]\n",
    "            overlyingHeaderNumeration += headerNumerationPart + '.'\n",
    "            \n",
    "    return overlyingHeaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrutok import Token, Sentence, Tokenizer, GraphemTag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def countHeadingWeight(headersAndTextHierarchy, requiredHeader, overlyingHeaders, requiredSentence, wordFrequencyWeights):\n",
    "    morphAnalyzer = MorphAnalyzer()\n",
    "    headingWeight = 0\n",
    "    \n",
    "    requiredSentenceWords = set()\n",
    "    for us in unusualSymbols:\n",
    "        requiredSentence = requiredSentence.replace(us, '')\n",
    "    for sentence in Tokenizer(requiredSentence):\n",
    "        for token in sentence:\n",
    "            if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                if word not in stopwords.words('russian'):\n",
    "                    requiredSentenceWords.add(word)\n",
    "                    \n",
    "    if set(requiredHeader.split(' ', 1)[0]).issubset(numbersAndPoint):\n",
    "        requiredHeader = requiredHeader.split(' ', 1)[1]\n",
    "    \n",
    "    for sentence in Tokenizer(requiredHeader):\n",
    "        for token in sentence:\n",
    "            if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                if word not in stopwords.words('russian') and \\\n",
    "                   word in requiredSentenceWords:\n",
    "                    headingWeight += wordFrequencyWeights[word]\n",
    " \n",
    "    nestedBlocksNumber = 1\n",
    "    for overlyingHeader in overlyingHeaders[::-1]:\n",
    "        headerNumeration = overlyingHeader.split(' ', -1)[0]\n",
    "        nestedHierarchy = headersAndTextHierarchy\n",
    "        for headerNumerationPart in headerNumeration.split('.'):\n",
    "            nestedHierarchy = nestedHierarchy[headerNumerationPart][1]\n",
    "        nestedBlocksNumber = len(nestedHierarchy) * nestedBlocksNumber\n",
    "        for sentence in Tokenizer(overlyingHeader):\n",
    "            for token in sentence:\n",
    "                if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                    word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                    if word not in stopwords.words('russian') and \\\n",
    "                       word in requiredSentenceWords:\n",
    "                        headingWeight += wordFrequencyWeights[word] / nestedBlocksNumber\n",
    "                        \n",
    "    return headingWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for level, headersAndSentences in headersAndSentencesByLevels.items():\n",
    "    for headerAndSentences in headersAndSentences:\n",
    "        headerAndSentences['overlyingHeaders'] = determineOverlyingHeaders(\n",
    "            headersAndTextHierarchy, \n",
    "            headerAndSentences['header']\n",
    "        )\n",
    "        \n",
    "for level, headersAndSentences in headersAndSentencesByLevels.items():\n",
    "    for blockIndex in range(0, len(headersAndSentences)):\n",
    "        for sentence in headersAndSentences[blockIndex]['sentences']:\n",
    "            sentence['headingWeight'] = countHeadingWeight(\n",
    "                headersAndTextHierarchy, \n",
    "                headersAndSentences[blockIndex]['header'], \n",
    "                headersAndSentences[blockIndex]['overlyingHeaders'], \n",
    "                sentence['sentence'], \n",
    "                headersAndWordFrequencyWeightsByLevels[level][blockIndex]['wordFrequencyWeights']\n",
    "            ) # * sentence['importance']\n",
    "        \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndSentencesByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вес предложения на определенном уровне иерархии: признак контрольных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonControlWords1 = ['метод', 'задача', 'решение', 'функция', 'система', 'результат', 'модель', 'алгоритм', 'параметр', 'работа', 'схема', 'сетка', 'вектор', 'шаг', 'матрица', 'численный', 'слой', 'поверхность', 'процесс', 'качество', 'зависимость', 'аппроксимация', 'преобразование', 'моделирование', 'граничный', 'оператор', 'анализ', 'линейный', 'сеть', 'постановка', 'распределение', 'этап', 'обучение', 'проблема', 'эксперимент', 'построение', 'сравнение', 'интеграл', 'точность', 'признак', 'множество', 'оценка', 'интегральный', 'программа', 'разностный', 'набор', 'реализация', 'погрешность', 'нелинейный', 'интерполяция']\n",
    "commonControlWords2 = ['задача', 'модель', 'функция', 'метод', 'алгоритм', 'система', 'работа', 'множество', 'решение', 'параметр', 'матрица', 'распределение', 'результат', 'образ', 'оценка', 'качество', 'уравнение', 'оптимальный', 'вектор', 'класс', 'объект', 'сеть', 'граф', 'набор', 'состояние', 'процесс', 'построение', 'шаг', 'схема', 'вероятность', 'элемент', 'выборка', 'анализ', 'признак', 'обучение', 'зависимость', 'сложность', 'вычисление', 'подход', 'эксперимент', 'ограничение', 'вход', 'реализация', 'свойство', 'этап', 'сравнение', 'дерево', 'оптимизация', 'итерация', 'проблема']\n",
    "commonControlWords3 = ['работа', 'алгоритм', 'метод', 'задача', 'система', 'результат', 'модель', 'сеть', 'анализ', 'решение', 'качество', 'функция', 'признак', 'набор', 'оценка', 'обучение', 'программа', 'класс', 'подход', 'параметр', 'вектор', 'граф', 'реализовать', 'объект', 'процесс', 'построение', 'элемент', 'архитектура', 'модуль', 'эксперимент', 'сервер', 'тестирование', 'программный', 'разработать', 'выборка', 'матрица', 'слой', 'цель', 'обработка', 'зависимость', 'база', 'классификация', 'проблема', 'схема', 'характеристика', 'этап', 'тестовый', 'вывод', 'разработка', 'шаг']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrutok import Token, Sentence, Tokenizer, GraphemTag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "\n",
    "def countSentenceControlFeatureByLevels(requiredSentence, wordsFrequencyWeight, tfIdfImportantWords):\n",
    "    morphAnalyzer = MorphAnalyzer()\n",
    "    sentenceWeight = 0\n",
    "    for us in unusualSymbols:\n",
    "        requiredSentence = requiredSentence.replace(us, '')\n",
    "    for sentence in Tokenizer(requiredSentence):\n",
    "        for token in sentence:\n",
    "            if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                if word not in stopwords.words('russian'):\n",
    "                    sentenceWeight += wordsFrequencyWeight[word] if word in commonControlWords2 else 0\n",
    "    return sentenceWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for level, headersAndSentences in headersAndSentencesByLevels.items():\n",
    "    for blockIndex in range(0, len(headersAndSentences)):\n",
    "        for sentence in headersAndSentences[blockIndex]['sentences']:\n",
    "            sentenceWeight = countSentenceControlFeatureByLevels(\n",
    "                sentence['sentence'], \n",
    "                headersAndWordFrequencyWeightsByLevels[level][blockIndex]['wordFrequencyWeights'],\n",
    "                tfIdfImportantWords\n",
    "            )\n",
    "            sentence['controlWeight'] = sentenceWeight # * sentence['importance']\n",
    "    \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndSentencesByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Извлекаем все веса, для всех разделов, чтобы потом их нормализовать, сложить и отсортировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for level, headersAndSentences in headersAndSentencesByLevels.items():\n",
    "    if level == 0:\n",
    "        continue\n",
    "    for blockIndex in range(0, len(headersAndSentences)):\n",
    "        headersAndSentences[blockIndex]['frequencyWeights'] = []\n",
    "        headersAndSentences[blockIndex]['locationWeights'] = []\n",
    "        headersAndSentences[blockIndex]['headingWeights'] = []\n",
    "        headersAndSentences[blockIndex]['controlWeights'] = []\n",
    "        for sentence in headersAndSentences[blockIndex]['sentences']:\n",
    "            headersAndSentences[blockIndex]['frequencyWeights'].append(sentence['frequencyWeight'])\n",
    "            headersAndSentences[blockIndex]['locationWeights'].append(sentence['locationWeight'])\n",
    "            headersAndSentences[blockIndex]['headingWeights'].append(sentence['headingWeight'])\n",
    "            headersAndSentences[blockIndex]['controlWeights'].append(sentence['controlWeight'])\n",
    "   \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndSentencesByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нормализуем, складываем и сортируем веса предложений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for level, headersAndSentences in headersAndSentencesByLevels.items():\n",
    "    if level == 0:\n",
    "        continue\n",
    "    for blockIndex in range(0, len(headersAndSentences)):\n",
    "        maxFrequencyWeight = max(headersAndSentences[blockIndex]['frequencyWeights']) if headersAndSentences[blockIndex]['frequencyWeights'] else 0\n",
    "        maxFrequencyWeight = maxFrequencyWeight if maxFrequencyWeight != 0 else 1\n",
    "        maxLocationWeight = max(headersAndSentences[blockIndex]['locationWeights']) if headersAndSentences[blockIndex]['locationWeights'] else 0\n",
    "        maxLocationWeight = maxLocationWeight if maxLocationWeight != 0 else 1\n",
    "        maxHeadingWeight = max(headersAndSentences[blockIndex]['headingWeights']) if headersAndSentences[blockIndex]['headingWeights'] else 0\n",
    "        maxHeadingWeight = maxHeadingWeight if maxHeadingWeight != 0 else 1\n",
    "        maxControlWeight = max(headersAndSentences[blockIndex]['controlWeights']) if headersAndSentences[blockIndex]['controlWeights'] else 0\n",
    "        maxControlWeight = maxControlWeight if maxControlWeight != 0 else 1\n",
    "        headersAndSentences[blockIndex]['weights'] = []\n",
    "        for sentence in headersAndSentences[blockIndex]['sentences']:\n",
    "            sentence['normalizedFrequencyWeight'] = sentence['frequencyWeight'] / maxFrequencyWeight\n",
    "            sentence['normalizedLocationWeight'] = sentence['locationWeight'] / maxLocationWeight\n",
    "            sentence['normalizedHeadingWeight'] = sentence['headingWeight'] / maxHeadingWeight\n",
    "            sentence['normalizedControlWeight'] = sentence['controlWeight'] / maxControlWeight\n",
    "            sentence['weight'] = (sentence['normalizedFrequencyWeight'] + \\\n",
    "                                  sentence['normalizedLocationWeight'] + \\\n",
    "                                  sentence['normalizedHeadingWeight'] + \\\n",
    "                                  sentence['normalizedControlWeight']) * \\\n",
    "                                 sentence['importance']\n",
    "            headersAndSentences[blockIndex]['weights'].append(sentence['weight'])\n",
    "#             del sentence['frequencyWeight']\n",
    "#             del sentence['locationWeight']\n",
    "#             del sentence['headingWeight']\n",
    "#             del sentence['controlWeight']\n",
    "#             del sentence['importance']\n",
    "        headersAndSentences[blockIndex]['weights'].sort(reverse = True)\n",
    "#         del headersAndSentences[blockIndex]['frequencyWeights']\n",
    "#         del headersAndSentences[blockIndex]['locationWeights']\n",
    "#         del headersAndSentences[blockIndex]['headingWeights']\n",
    "#         del headersAndSentences[blockIndex]['controlWeights']\n",
    "    \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(headersAndSentencesByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделяем по пять значимых предложений на раздел.\n",
    "\n",
    "Этого достаточно с учетом длины предложений и возможного наличия картинок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "importantSentencesByLevels = {}\n",
    "for level, headersAndSentences in headersAndSentencesByLevels.items():\n",
    "    if level == 0:\n",
    "        continue\n",
    "    importantSentencesByLevels[level] = []\n",
    "    for headerAndSentences in headersAndSentences:\n",
    "        importantSentencesWeights = [weight for weight in headerAndSentences['weights'] if weight > 0][:5]\n",
    "        importantSentences = []\n",
    "        imageReferences = set()\n",
    "        containsBulletList = False\n",
    "        containsNumberedList = False\n",
    "        for sentence in headerAndSentences['sentences']:\n",
    "            if sentence['weight'] in importantSentencesWeights:\n",
    "                importantSentences.append({\n",
    "                    'sentence': sentence['sentence'],\n",
    "                    'betweenBullets': sentence['betweenBullets'],\n",
    "                    'bulletListPart': sentence['bulletListPart'],\n",
    "                    'betweenNumbering': sentence['betweenNumbering'],\n",
    "                    'numberedListPart': sentence['numberedListPart'],\n",
    "                    'imageReferences': sentence['imageReferences'],\n",
    "                    'tableReferences': sentence['tableReferences']\n",
    "                })\n",
    "                imageReferences = imageReferences.union(sentence['imageReferences'])\n",
    "                if (sentence['bulletListPart']):\n",
    "                    containsBulletList = True\n",
    "                if (sentence['numberedListPart']):\n",
    "                    containsNumberedList = True\n",
    "        importantSentencesByLevels[level].append({\n",
    "            'header': headerAndSentences['header'],\n",
    "            'overlyingHeaders': headerAndSentences['overlyingHeaders'],\n",
    "            'importantSentences': importantSentences,\n",
    "            'sentencesImageReferences': imageReferences,\n",
    "            'imageReferences':  headerAndSentences['imageReferences'] if 'imageReferences' in headerAndSentences else set(),\n",
    "            'tableReferences': headerAndSentences['tableReferences'] if 'tableReferences' in headerAndSentences else set(),\n",
    "            'containsBulletList': containsBulletList,\n",
    "            'containsNumberedList': containsNumberedList\n",
    "        })\n",
    "     \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(importantSentencesByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оцениваем количество слайдов для каждого заголовка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если в разделе есть таблица, то она получает отдельный слайд\n",
    "# если в разделе есть рисунок и он единственный, то он получает отдельный слайд\n",
    "# на каждый заголовок гарантированно приходится хотя бы один слайд\n",
    "# изображение, ссылка на которое есть в тексте, получает отдельный слайд, если отношение его высоты к его ширине меньше 1.5 и больше 0.3\n",
    "# изображение, ссылка на которое есть в тексте, получает отдельный слайд, если колиество изображений с одной подписью больше одного\n",
    "\n",
    "for level, headersAndSentences in importantSentencesByLevels.items():\n",
    "    for headerAndSentences in headersAndSentences:\n",
    "        slidesCount = 1\n",
    "        slidesCount += len(headerAndSentences['tableReferences'])\n",
    "        slidesCount += 1 if len(headerAndSentences['imageReferences']) == 1 and \\\n",
    "                            len(headerAndSentences['imageReferences'].difference(headerAndSentences['sentencesImageReferences'])) == 1 \\\n",
    "                         else 0\n",
    "        imageSlidesCount = 0\n",
    "        for imageNumeration in headerAndSentences['sentencesImageReferences']:\n",
    "            imageWithSameCaption = [caption for caption in captionsToImages\n",
    "                                    if caption['numeration'] == imageNumeration]\n",
    "            if len(imageWithSameCaption) > 1:\n",
    "                imageSlidesCount += 1\n",
    "            else:\n",
    "                image = imageWithSameCaption[0]\n",
    "                heightToWidthRatio = image['height'] / image['width']\n",
    "                if heightToWidthRatio > 0.3 and heightToWidthRatio < 1.5:\n",
    "                    imageSlidesCount += 1\n",
    "        # это на случай, если в извлеченном тексте слишком много ссылок на изображения\n",
    "        if len(headerAndSentences['sentencesImageReferences']) - imageSlidesCount > 1:\n",
    "            imageSlidesCount = len(headerAndSentences['sentencesImageReferences']) - 1\n",
    "        headerAndSentences['slidesCount'] = slidesCount + imageSlidesCount\n",
    "        \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(importantSentencesByLevels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определяем важность заголовка по наличию или отсутствию в нем ключевых слов. Оцениваем количество слайдов для каждого уровня важности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantHeadersMarkers = ['результат', 'введение', 'вывод', 'метод', 'подход', 'заключение']\n",
    "importantHeadersBigrams = ['постановка задача']\n",
    "\n",
    "unimportantHeadersMarkers = ['обзор', 'приложение', 'литература']\n",
    "unimportantHeadersBigrams = ['список литература']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrutok import Token, Sentence, Tokenizer, GraphemTag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "\n",
    "absolutelyImportantHeaders = []\n",
    "absolutelyImportantSlidesCount = 0\n",
    "highImportantHeaders = []\n",
    "highImportantSlidesCount = 0\n",
    "mediumImportantHeaders = []\n",
    "mediumImportantSlidesCount = 0\n",
    "tableReferences = set()\n",
    "for level, headersAndSentences in importantSentencesByLevels.items():\n",
    "    if level == 0:\n",
    "        continue\n",
    "    for headerAndSentences in headersAndSentences:\n",
    "        header = headerAndSentences['header']\n",
    "        headerWords = []\n",
    "        for sentence in Tokenizer(header):\n",
    "            for token in sentence:\n",
    "                if GraphemTag.contains(token.get_graphem_tag(), GraphemTag.CYRILLIC):\n",
    "                    word = morphAnalyzer.parse(token.get_escaped_data())[0].normal_form\n",
    "                    if word not in stopwords.words('russian'):\n",
    "                        headerWords.append(word)\n",
    "        headerBigrams = [headerWords[i] + ' ' + headerWords[i+1] for i in range(0, len(headerWords) - 1)]\n",
    "#         print(headerWords)\n",
    "        if ('приложение' in headerWords) or \\\n",
    "           ('литература' in headerWords and len(headerWords) == 1) or \\\n",
    "           ('список литература' in headerBigrams and len(headerBigrams) == 1) or \\\n",
    "           'алгоритм' in headerWords:\n",
    "            # разделы, которые совсем не нужны\n",
    "            pass\n",
    "        elif ('введение' in headerWords or 'заключение' in headerWords or 'вывод' in headerWords) or \\\n",
    "             'постановка задача' in headerBigrams:\n",
    "#              headerAndSentences['containsBulletList'] or headerAndSentences['containsNumberedList']:\n",
    "            tableSlides = len(headerAndSentences['tableReferences'].intersection(tableReferences))\n",
    "            tableReferences = tableReferences.union(headerAndSentences['tableReferences'])\n",
    "            absolutelyImportantHeaders.append(header)\n",
    "            absolutelyImportantSlidesCount += headerAndSentences['slidesCount'] - tableSlides\n",
    "            highImportantHeaders.append(header)\n",
    "            highImportantSlidesCount += headerAndSentences['slidesCount'] - tableSlides\n",
    "            mediumImportantHeaders.append(header)\n",
    "            mediumImportantSlidesCount += headerAndSentences['slidesCount'] - tableSlides\n",
    "        elif 'результат' in headerWords or 'метод' in headerWords or \\\n",
    "             'подход' in headerWords or 'система' in headerWords or \\\n",
    "             'метрика' in headerWords or 'реализация' in headerWords or \\\n",
    "             headerAndSentences['tableReferences']:\n",
    "#             print(headerAndSentences['header'], headerAndSentences['tableReferences'])\n",
    "            tableSlides = len(headerAndSentences['tableReferences'].intersection(tableReferences))\n",
    "            tableReferences = tableReferences.union(headerAndSentences['tableReferences'])\n",
    "            highImportantHeaders.append(header)\n",
    "            highImportantSlidesCount += headerAndSentences['slidesCount'] - tableSlides\n",
    "            mediumImportantHeaders.append(header)\n",
    "            mediumImportantSlidesCount += headerAndSentences['slidesCount'] - tableSlides\n",
    "        elif 'обзор' in headerWords or \\\n",
    "             ('приложение' in headerWords and len(headerWords) == 1) or \\\n",
    "             ('литература' in headerWords and len(headerWords) == 1) or \\\n",
    "             ('список литература' in headerBigrams and len(headerBigrams) == 1):\n",
    "            # разделы, которые совсем не нужны\n",
    "            pass\n",
    "        else:\n",
    "            tableSlides = len(headerAndSentences['tableReferences'].intersection(tableReferences))\n",
    "            tableReferences = tableReferences.union(headerAndSentences['tableReferences'])\n",
    "            mediumImportantHeaders.append(header)\n",
    "            mediumImportantSlidesCount += headerAndSentences['slidesCount'] - tableSlides\n",
    "            \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    print(absolutelyImportantHeaders)\n",
    "    print(absolutelyImportantSlidesCount)\n",
    "    print(highImportantHeaders)\n",
    "    print(highImportantSlidesCount)\n",
    "    print(mediumImportantHeaders)\n",
    "    print(mediumImportantSlidesCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presentationSlidesCount = 0\n",
    "presentationHeaders = []\n",
    "if absolutelyImportantSlidesCount > 10 and absolutelyImportantSlidesCount <= 16:\n",
    "    presentationSlidesCount = absolutelyImportantSlidesCount\n",
    "    presentationHeaders = absolutelyImportantHeaders\n",
    "if highImportantSlidesCount > 10 and highImportantSlidesCount <= 16:\n",
    "    presentationSlidesCount = highImportantSlidesCount\n",
    "    presentationHeaders = highImportantHeaders\n",
    "if mediumImportantSlidesCount > 10 and mediumImportantSlidesCount <= 16:\n",
    "    presentationSlidesCount = mediumImportantSlidesCount\n",
    "    presentationHeaders = mediumImportantHeaders\n",
    "    \n",
    "presentationContent = []\n",
    "for level, headersAndSentences in importantSentencesByLevels.items():\n",
    "    for headerAndSentences in headersAndSentences:\n",
    "        if headerAndSentences['header'] in presentationHeaders:\n",
    "            if set(headerAndSentences['header'].split(' ', 1)[0]).issubset(numbersAndPoint): \n",
    "                header = headerAndSentences['header'].split(' ', 1)[1]\n",
    "            else:\n",
    "                header = headerAndSentences['header']\n",
    "            presentationContent.append({\n",
    "                'header': headerAndSentences['header'],\n",
    "                'bareHeader': header,\n",
    "                'importantSentences': headerAndSentences['importantSentences'],\n",
    "                'sectionImageReferences': headerAndSentences['imageReferences']\n",
    "                                          if len(headerAndSentences['imageReferences']) == 1 and \\\n",
    "                                             len(headerAndSentences['imageReferences'].difference(headerAndSentences['sentencesImageReferences'])) == 1\n",
    "                                          else set(),\n",
    "                'tableReferences': headerAndSentences['tableReferences']\n",
    "            })\n",
    "            \n",
    "presentationContent = sorted(presentationContent, key = lambda h: headersByOrder.index(h['header']))\n",
    "            \n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    pprint(presentationContent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
